{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **GCN-based Heterogeneous Graph Recommendation System**"
      ],
      "metadata": {
        "id": "CDsSjG9kpPBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPkPc9TCHx_I",
        "outputId": "14c9a1ce-93e2-40fc-8a93-dc6541ede0e2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import ast\n",
        "from collections import defaultdict\n",
        "import zipfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import torch\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.nn import HeteroConv, SAGEConv, RGCNConv, Linear, GraphConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    ndcg_score,\n",
        ")\n",
        "from typing import Dict, Any, List\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "sns.set_palette(sns.color_palette('CMRmap'))\n",
        "\n",
        "path = 'graph_data'\n",
        "\n",
        "with zipfile.ZipFile('graph_data.zip', 'r') as zf:\n",
        "    zf.extractall(path)\n"
      ],
      "metadata": {
        "id": "-01eSZtI02bR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preparation**\n",
        "\n",
        "Load all node features and edge lists for heterogeneous graph construction"
      ],
      "metadata": {
        "id": "rpGfiKBCphJa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yIq9xvSw0oqu"
      },
      "outputs": [],
      "source": [
        "df_connections_user_stats = pd.read_csv(f'{path}/user_stats.csv')\n",
        "df_connections_movie_features = pd.read_csv(f'{path}/movie_features.csv')\n",
        "df_connections_movie_genre = pd.read_csv(f'{path}/movie_genre_edges.csv')\n",
        "df_connections_movie_actor = pd.read_csv(f'{path}/movie_actor_edges.csv')\n",
        "df_connections_movie_director = pd.read_csv(f'{path}/movie_director_edges.csv')\n",
        "df_connections_user_movie = pd.read_csv(f'{path}/user_movie_edges.csv')\n",
        "df_connections_actors = pd.read_csv(f'{path}/actors.csv')\n",
        "df_connections_directors = pd.read_csv(f'{path}/directors.csv')\n",
        "df_connections_genres = pd.read_csv(f'{path}/genres.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Movies: {df_connections_movie_features.shape}')\n",
        "print(f'Users: {df_connections_user_stats.shape}')\n",
        "print(f'Directors: {df_connections_directors.shape}')\n",
        "print(f'Actors: {df_connections_actors.shape}')\n",
        "print(f'Genres: {df_connections_genres.shape}')\n",
        "print(f'Movie-genre links: {df_connections_movie_genre.shape}')\n",
        "print(f'Movie-actor links: {df_connections_movie_actor.shape}')\n",
        "print(f'Movie-director links: {df_connections_movie_director.shape}')\n",
        "print(f'User-movie links: {df_connections_user_movie.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcZHkPl2K3se",
        "outputId": "dff8b597-ae65-4edf-c03b-8e96923dba5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Movies: (16409, 4)\n",
            "Users: (75680, 4)\n",
            "Directors: (1741, 3)\n",
            "Actors: (2517, 4)\n",
            "Genres: (20, 2)\n",
            "Movie-genre links: (38491, 2)\n",
            "Movie-actor links: (48106, 4)\n",
            "Movie-director links: (10570, 2)\n",
            "User-movie links: (2505904, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create mapping dictionaries: original IDs -> consecutive indices for all node types"
      ],
      "metadata": {
        "id": "HR2BbHPIpy2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_id_to_idx = {movie_id: idx for idx, movie_id in enumerate(df_connections_movie_features['movie_id'].unique())}\n",
        "\n",
        "user_id_to_idx = {user_id: idx for idx, user_id in enumerate(df_connections_user_stats['user_id'].unique())}\n",
        "\n",
        "director_id_to_idx = {director_id: idx for idx, director_id in\n",
        "                      enumerate(df_connections_directors['director_id'].unique())}\n",
        "\n",
        "actor_id_to_idx = {actor_id: idx for idx, actor_id in enumerate(df_connections_actors['actor_id'].unique())}\n",
        "\n",
        "genre_id_to_idx = {genre_id: idx for idx, genre_id in enumerate(df_connections_genres['genre_id'].unique())}\n",
        "\n",
        "print(f'Number of movies: {len(movie_id_to_idx)}')\n",
        "print(f'Number of users: {len(user_id_to_idx)}')\n",
        "print(f'Number of directors: {len(director_id_to_idx)}')\n",
        "print(f'Number of actors: {len(actor_id_to_idx)}')\n",
        "print(f'Number of genres: {len(genre_id_to_idx)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z0IpddTMImt",
        "outputId": "b351740a-d6a0-4c56-edb4-9566f6b3c6d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of movies: 16409\n",
            "Number of users: 75680\n",
            "Number of directors: 1741\n",
            "Number of actors: 2517\n",
            "Number of genres: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Subgraph Construction Pipeline**\n",
        "\n",
        "This function builds a heterogeneous graph subgraph centered around selected users, preparing data for GraphSAGE training.\n",
        "\n",
        "### **Main Parameters**\n",
        "- **`user_ids`**: Target users to build the subgraph around (central nodes)\n",
        "- **`num_negatives`**: Number of negative samples for training (200 by default) - affects training data balance\n",
        "- **DataFrames**: Feature tables and edge lists for all entity types\n",
        "\n",
        "### **Pipeline Stages**\n",
        "\n",
        "#### **1. Data Filtering & Propagation**\n",
        "- **Step 1**: Filter user-related data based on `user_ids`\n",
        "- **Step 2**: Propagate filtering to connected movies via user-movie edges\n",
        "- **Step 3**: Further propagate to related entities (actors, directors, genres)\n",
        "- **Result**: A connected subgraph containing only relevant entities\n",
        "\n",
        "#### **2. Feature Engineering & Normalization**\n",
        "- **Numerical features**: Standardized using `StandardScaler` for:\n",
        "  - Movie metrics (`vote_average`, `vote_count`, `popularity`)\n",
        "  - User statistics (`num_ratings`, `avg_rating`, `activity_days`)\n",
        "  - Actor/director metrics (`total_films`, `avg_order`)\n",
        "- **Categorical encoding**:\n",
        "  - Gender (directors/actors): One-hot encoded (3 categories: 0,1,2)\n",
        "  - Genres: One-hot encoded per unique genre ID\n",
        "\n",
        "#### **3. ID Remapping & Graph Initialization**\n",
        "- Creates sequential index mappings for each node type\n",
        "- Initializes `HeteroData` object with:\n",
        "  - `num_nodes` defined for each entity type\n",
        "  - Maintains original ID to index mapping for reference\n",
        "\n",
        "#### **4. Node Feature Assignment**\n",
        "- For each entity type, extracts normalized features\n",
        "- Maps features to corresponding node indices\n",
        "\n",
        "#### **5. Edge Construction**\n",
        "- Builds five relationship types with optional weights:\n",
        "  1. `('user', 'rates', 'movie')` - with rating weights\n",
        "  2. `('movie', 'has_genre', 'genre')`\n",
        "  3. `('movie', 'has_director', 'director')`\n",
        "  4. `('movie', 'has_actor', 'actor')` - with role weight\n",
        "\n",
        "#### **6. Bidirectional Edge Addition**\n",
        "- Creates reverse edges for message passing in both directions\n",
        "- Reverse edges follow pattern: `('genre', 'rev_has_genre', 'movie')`\n",
        "\n",
        "### **Output Structure**\n",
        "Returns a tuple containing:\n",
        "1. **`data`**: PyG HeteroData object\n",
        "2. **`mappings`**: ID-to-index dictionaries for all entity types\n",
        "3. **`df_user_movie_sub`**: Filtered user-movie interactions for training"
      ],
      "metadata": {
        "id": "GsFfkyZJKdjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_subgraph_for_users(\n",
        "        user_ids,\n",
        "        df_user_movie,\n",
        "        df_movie_features,\n",
        "        df_user_stats,\n",
        "        df_movie_genre,\n",
        "        df_movie_actor,\n",
        "        df_movie_director,\n",
        "        df_actors,\n",
        "        df_directors,\n",
        "        df_genres,\n",
        "        num_negatives=200\n",
        "):\n",
        "    selected_users = set(user_ids)\n",
        "    df_user_stats_sub = df_user_stats[df_user_stats['user_id'].isin(selected_users)].copy()\n",
        "\n",
        "    df_user_movie_sub = df_user_movie[df_user_movie['user_id'].isin(selected_users)].copy()\n",
        "    selected_movie_ids = set(df_user_movie_sub['movie_id'].unique())\n",
        "\n",
        "    df_movie_features_sub = df_movie_features[df_movie_features['movie_id'].isin(selected_movie_ids)].copy()\n",
        "    df_movie_genre_sub = df_movie_genre[df_movie_genre['movie_id'].isin(selected_movie_ids)].copy()\n",
        "    df_movie_actor_sub = df_movie_actor[df_movie_actor['movie_id'].isin(selected_movie_ids)].copy()\n",
        "    df_movie_director_sub = df_movie_director[df_movie_director['movie_id'].isin(selected_movie_ids)].copy()\n",
        "\n",
        "    selected_genre_ids = set(df_movie_genre_sub['genre_id'].unique())\n",
        "    selected_actor_ids = set(df_movie_actor_sub['actor_id'].unique())\n",
        "    selected_director_ids = set(df_movie_director_sub['director_id'].unique())\n",
        "\n",
        "    df_genres_sub = df_genres[df_genres['genre_id'].isin(selected_genre_ids)].copy()\n",
        "    df_actors_sub = df_actors[df_actors['actor_id'].isin(selected_actor_ids)].copy()\n",
        "    df_directors_sub = df_directors[df_directors['director_id'].isin(selected_director_ids)].copy()\n",
        "\n",
        "    print(f'\\nSizes of filtered data:')\n",
        "    print(f'Users: {df_user_stats_sub.shape[0]}')\n",
        "    print(f'Movies: {df_movie_features_sub.shape[0]}')\n",
        "    print(f'User-movie links: {df_user_movie_sub.shape[0]}')\n",
        "    print(f'Actors: {df_actors_sub.shape[0]}')\n",
        "    print(f'Directors: {df_directors_sub.shape[0]}')\n",
        "    print(f'Genres: {df_genres_sub.shape[0]}')\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    movie_num_cols = ['vote_average', 'vote_count', 'popularity']\n",
        "    if len(df_movie_features_sub) > 0:\n",
        "        df_movie_features_sub[movie_num_cols] = scaler.fit_transform(\n",
        "            df_movie_features_sub[movie_num_cols].fillna(0)\n",
        "        )\n",
        "\n",
        "    user_num_cols = ['num_ratings', 'avg_rating', 'activity_days']\n",
        "    if len(df_user_stats_sub) > 0:\n",
        "        df_user_stats_sub[user_num_cols] = scaler.fit_transform(\n",
        "            df_user_stats_sub[user_num_cols].fillna(0)\n",
        "        )\n",
        "\n",
        "    if len(df_directors_sub) > 0:\n",
        "        df_directors_sub['total_films'] = scaler.fit_transform(\n",
        "            df_directors_sub[['total_films']].fillna(0)\n",
        "        )\n",
        "\n",
        "    actor_num_cols = ['total_films', 'avg_order']\n",
        "    if len(df_actors_sub) > 0:\n",
        "        df_actors_sub[actor_num_cols] = scaler.fit_transform(\n",
        "            df_actors_sub[actor_num_cols].fillna(0)\n",
        "        )\n",
        "\n",
        "    gender_encoder = OneHotEncoder(sparse_output=False, categories=[[0, 1, 2]], handle_unknown='ignore')\n",
        "    if len(df_directors_sub) > 0:\n",
        "        gender_dir_encoded = gender_encoder.fit_transform(df_directors_sub[['gender']])\n",
        "        gender_dir_df = pd.DataFrame(gender_dir_encoded,\n",
        "                                     columns=['gender_0', 'gender_1', 'gender_2'])\n",
        "        df_directors_sub = pd.concat([df_directors_sub.reset_index(drop=True),\n",
        "                                      gender_dir_df], axis=1)\n",
        "        df_directors_sub = df_directors_sub.drop('gender', axis=1)\n",
        "\n",
        "    if len(df_actors_sub) > 0:\n",
        "        gender_act_encoded = gender_encoder.transform(df_actors_sub[['gender']])\n",
        "        gender_act_df = pd.DataFrame(gender_act_encoded,\n",
        "                                     columns=['gender_0', 'gender_1', 'gender_2'])\n",
        "        df_actors_sub = pd.concat([df_actors_sub.reset_index(drop=True),\n",
        "                                   gender_act_df], axis=1)\n",
        "        df_actors_sub = df_actors_sub.drop('gender', axis=1)\n",
        "\n",
        "    if len(df_genres_sub) > 0:\n",
        "        genre_encoder = OneHotEncoder(sparse_output=False)\n",
        "        genre_encoded = genre_encoder.fit_transform(df_genres_sub[['genre_id']])\n",
        "        genre_cols = [f'genre_{int(i)}' for i in genre_encoder.categories_[0]]\n",
        "        genre_df = pd.DataFrame(genre_encoded, columns=genre_cols)\n",
        "        df_genres_sub = pd.concat([df_genres_sub.reset_index(drop=True),\n",
        "                                   genre_df], axis=1)\n",
        "        df_genres_sub = df_genres_sub.drop('genre_name', axis=1)\n",
        "\n",
        "    mappings = {}\n",
        "    mappings['user'] = {user_id: idx for idx, user_id in enumerate(df_user_stats_sub['user_id'].unique())}\n",
        "    mappings['movie'] = {movie_id: idx for idx, movie_id in enumerate(df_movie_features_sub['movie_id'].unique())}\n",
        "    mappings['actor'] = {actor_id: idx for idx, actor_id in enumerate(df_actors_sub['actor_id'].unique())}\n",
        "    mappings['director'] = {director_id: idx for idx, director_id in\n",
        "                            enumerate(df_directors_sub['director_id'].unique())}\n",
        "    mappings['genre'] = {genre_id: idx for idx, genre_id in enumerate(df_genres_sub['genre_id'].unique())}\n",
        "\n",
        "    data = HeteroData()\n",
        "    data['user'].num_nodes = len(mappings['user'])\n",
        "    data['movie'].num_nodes = len(mappings['movie'])\n",
        "    data['actor'].num_nodes = len(mappings['actor'])\n",
        "    data['director'].num_nodes = len(mappings['director'])\n",
        "    data['genre'].num_nodes = len(mappings['genre'])\n",
        "\n",
        "    def add_node_features(df, node_type, id_col, mapping, data_obj):\n",
        "        if len(df) == 0:\n",
        "            return\n",
        "\n",
        "        features_list = []\n",
        "        feature_cols = [col for col in df.columns if col != id_col]\n",
        "        sorted_ids = sorted(mapping.keys(), key=lambda x: mapping[x])\n",
        "        for node_id in sorted_ids:\n",
        "            row = df[df[id_col] == node_id]\n",
        "            if len(row) > 0:\n",
        "                features = row[feature_cols].values[0].astype(np.float32)\n",
        "                features_list.append(features)\n",
        "            else:\n",
        "                features_list.append(np.zeros(len(feature_cols), dtype=np.float32))\n",
        "        data_obj[node_type].x = torch.tensor(np.array(features_list), dtype=torch.float32)\n",
        "\n",
        "    add_node_features(df_user_stats_sub, 'user', 'user_id', mappings['user'], data)\n",
        "    add_node_features(df_movie_features_sub, 'movie', 'movie_id', mappings['movie'], data)\n",
        "    add_node_features(df_actors_sub, 'actor', 'actor_id', mappings['actor'], data)\n",
        "    add_node_features(df_directors_sub, 'director', 'director_id', mappings['director'], data)\n",
        "    add_node_features(df_genres_sub, 'genre', 'genre_id', mappings['genre'], data)\n",
        "\n",
        "    def add_edges(df, source_type, target_type, source_col, target_col,\n",
        "                  edge_type_name, data_obj, mappings_dict, weight_col=None):\n",
        "        if len(df) == 0:\n",
        "            return\n",
        "\n",
        "        edges = []\n",
        "        weights = [] if weight_col is not None else None\n",
        "        for _, row in df.iterrows():\n",
        "            source_idx = mappings_dict[source_type].get(row[source_col])\n",
        "            target_idx = mappings_dict[target_type].get(row[target_col])\n",
        "            if source_idx is not None and target_idx is not None:\n",
        "                edges.append([source_idx, target_idx])\n",
        "                if weight_col is not None:\n",
        "                    weights.append(row[weight_col])\n",
        "\n",
        "        if edges:\n",
        "            edge_tensor = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "            data_obj[source_type, edge_type_name, target_type].edge_index = edge_tensor\n",
        "            if weights is not None:\n",
        "                data_obj[source_type, edge_type_name, target_type].edge_weight = torch.tensor(\n",
        "                    weights, dtype=torch.float32\n",
        "                )\n",
        "\n",
        "    add_edges(df_user_movie_sub, 'user', 'movie', 'user_id', 'movie_id',\n",
        "              'rates', data, mappings, weight_col='rating')\n",
        "    add_edges(df_movie_genre_sub, 'movie', 'genre', 'movie_id', 'genre_id',\n",
        "              'has_genre', data, mappings)\n",
        "    add_edges(df_movie_director_sub, 'movie', 'director', 'movie_id', 'director_id',\n",
        "              'has_director', data, mappings)\n",
        "    add_edges(df_movie_actor_sub, 'movie', 'actor', 'movie_id', 'actor_id',\n",
        "              'has_actor', data, mappings, weight_col='weight')\n",
        "\n",
        "    edge_types_to_reverse = [\n",
        "        ('movie', 'has_genre', 'genre'),\n",
        "        ('movie', 'has_director', 'director'),\n",
        "        ('movie', 'has_actor', 'actor'),\n",
        "        ('user', 'rates', 'movie')\n",
        "    ]\n",
        "\n",
        "    for src, rel, dst in edge_types_to_reverse:\n",
        "        if hasattr(data[src, rel, dst], 'edge_index'):\n",
        "            reverse_edges = data[src, rel, dst].edge_index[[1, 0]]\n",
        "            rev_rel = f'rev_{rel}'\n",
        "            data[dst, rev_rel, src].edge_index = reverse_edges\n",
        "            if hasattr(data[src, rel, dst], 'edge_weight'):\n",
        "                data[dst, rev_rel, src].edge_weight = data[src, rel, dst].edge_weight\n",
        "\n",
        "    print(f'\\nConstructed graph with the following structure:')\n",
        "    print(f'Nodes: {data.node_types}')\n",
        "    print(f'Edges: {data.edge_types}')\n",
        "    print(f'\\nChecking node features:')\n",
        "\n",
        "    for node_type in data.node_types:\n",
        "        if hasattr(data[node_type], 'x'):\n",
        "            print(f'  {node_type}: {data[node_type].x.shape}')\n",
        "\n",
        "    print(f'\\nChecking edges:')\n",
        "    for edge_type in data.edge_types:\n",
        "        if hasattr(data[edge_type], 'edge_index'):\n",
        "            print(f'  {edge_type}: {data[edge_type].edge_index.shape[1]} edges')\n",
        "    return data, mappings, df_user_movie_sub\n"
      ],
      "metadata": {
        "id": "Xj_kfqjXVOAx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Active User Selection**\n",
        "\n",
        "This function identifies the most active users by rating count, ensuring the subgraph contains users with sufficient interaction data for reliable embeddings. Returns the top N user IDs sorted by activity level."
      ],
      "metadata": {
        "id": "jP9kuR8NAorN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_active_users(df_user_movie, df_user_stats, n_users=1000):\n",
        "    user_activity = df_user_movie.groupby('user_id').size().reset_index(name='rating_count')\n",
        "    user_activity = user_activity.sort_values('rating_count', ascending=False)\n",
        "    active_user_ids = user_activity.head(n_users)['user_id'].tolist()\n",
        "\n",
        "    print(f'Selected {len(active_user_ids)} most active users')\n",
        "    print(f'Minimum number of ratings: {user_activity.head(n_users)['rating_count'].min()}')\n",
        "    print(f'Maximum number of ratings: {user_activity.head(n_users)['rating_count'].max()}')\n",
        "    print(f'Average number of ratings: {user_activity.head(n_users)['rating_count'].mean():.2f}')\n",
        "\n",
        "    return active_user_ids\n"
      ],
      "metadata": {
        "id": "jukX4D09VaXn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stratified Train/Val/Test Split**\n",
        "\n",
        "Performs user-level stratified splitting while maintaining the positive (rating ≥4) vs negative interaction ratio for each user. This ensures consistent label distribution across all splits and prevents data leakage between users."
      ],
      "metadata": {
        "id": "1gdecNwBBIl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data_stratified(df_user_movie, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-9\n",
        "    train_dfs, val_dfs, test_dfs = [], [], []\n",
        "\n",
        "    for user_id, group in df_user_movie.groupby('user_id'):\n",
        "        positive = group[group['rating'] >= 4].copy()\n",
        "        negative = group[group['rating'] < 4].copy()\n",
        "\n",
        "        if len(positive) > 0:\n",
        "            pos_train, pos_temp = train_test_split(\n",
        "                positive, test_size=(val_ratio + test_ratio) / (val_ratio + test_ratio + train_ratio),\n",
        "                random_state=random_state\n",
        "            )\n",
        "            pos_val, pos_test = train_test_split(\n",
        "                pos_temp, test_size=test_ratio / (val_ratio + test_ratio),\n",
        "                random_state=random_state\n",
        "            )\n",
        "        else:\n",
        "            pos_train, pos_val, pos_test = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "        if len(negative) > 0:\n",
        "            neg_train, neg_temp = train_test_split(\n",
        "                negative, test_size=(val_ratio + test_ratio) / (val_ratio + test_ratio + train_ratio),\n",
        "                random_state=random_state\n",
        "            )\n",
        "            neg_val, neg_test = train_test_split(\n",
        "                neg_temp, test_size=test_ratio / (val_ratio + test_ratio),\n",
        "                random_state=random_state\n",
        "            )\n",
        "        else:\n",
        "            neg_train, neg_val, neg_test = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "        train_dfs.append(pd.concat([pos_train, neg_train]))\n",
        "        val_dfs.append(pd.concat([pos_val, neg_val]))\n",
        "        test_dfs.append(pd.concat([pos_test, neg_test]))\n",
        "\n",
        "    train_df = pd.concat(train_dfs, ignore_index=True)\n",
        "    val_df = pd.concat(val_dfs, ignore_index=True)\n",
        "    test_df = pd.concat(test_dfs, ignore_index=True)\n",
        "\n",
        "    print(f'Train: {len(train_df)} links ({len(train_df) / len(df_user_movie) * 100:.1f}%)')\n",
        "    print(f'Val: {len(val_df)} links ({len(val_df) / len(df_user_movie) * 100:.1f}%)')\n",
        "    print(f'Test: {len(test_df)} links ({len(test_df) / len(df_user_movie) * 100:.1f}%)')\n",
        "\n",
        "    return train_df, val_df, test_df\n"
      ],
      "metadata": {
        "id": "RXKLB78ZXPEM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Graph Split Mask Assignment**\n",
        "\n",
        "Maps the stratified user-movie splits back to the graph structure by creating binary masks for each edge in the `('user', 'rates', 'movie')` relation."
      ],
      "metadata": {
        "id": "Tjt1fbFgBXR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_split_masks_to_graph(graph, train_df, val_df, test_df, mappings):\n",
        "    train_set = set(zip(train_df['user_id'], train_df['movie_id']))\n",
        "    val_set = set(zip(val_df['user_id'], val_df['movie_id']))\n",
        "    test_set = set(zip(test_df['user_id'], test_df['movie_id']))\n",
        "\n",
        "    if ('user', 'rates', 'movie') in graph.edge_types:\n",
        "        edge_index = graph['user', 'rates', 'movie'].edge_index\n",
        "        num_edges = edge_index.shape[1]\n",
        "        train_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
        "        val_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
        "        test_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
        "\n",
        "        for i in range(num_edges):\n",
        "            user_idx = edge_index[0, i].item()\n",
        "            movie_idx = edge_index[1, i].item()\n",
        "            user_id = None\n",
        "            movie_id = None\n",
        "\n",
        "            for uid, uidx in mappings['user'].items():\n",
        "                if uidx == user_idx:\n",
        "                    user_id = uid\n",
        "                    break\n",
        "\n",
        "            for mid, midx in mappings['movie'].items():\n",
        "                if midx == movie_idx:\n",
        "                    movie_id = mid\n",
        "                    break\n",
        "\n",
        "            if user_id is not None and movie_id is not None:\n",
        "                edge_tuple = (user_id, movie_id)\n",
        "\n",
        "                if edge_tuple in train_set:\n",
        "                    train_mask[i] = True\n",
        "                elif edge_tuple in val_set:\n",
        "                    val_mask[i] = True\n",
        "                elif edge_tuple in test_set:\n",
        "                    test_mask[i] = True\n",
        "\n",
        "        graph['user', 'rates', 'movie'].train_mask = train_mask\n",
        "        graph['user', 'rates', 'movie'].val_mask = val_mask\n",
        "        graph['user', 'rates', 'movie'].test_mask = test_mask\n",
        "\n",
        "        print(f'Masks added:')\n",
        "        print(f'Train: {train_mask.sum().item()} edges')\n",
        "        print(f'Val: {val_mask.sum().item()} edges')\n",
        "        print(f'Test: {test_mask.sum().item()} edges')\n",
        "\n",
        "    return graph\n"
      ],
      "metadata": {
        "id": "awLi_VGrX5k2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Optimized Split Mask Assignment**\n",
        "\n",
        "Creates reverse ID mappings for efficient lookup and applies the same train/val/test masks to both directional and reverse user-movie edges, ensuring consistency during bidirectional message passing in GraphSAGE."
      ],
      "metadata": {
        "id": "AHtR3obLBkhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_split_masks_fast(graph, train_df, val_df, test_df, mappings):\n",
        "    reverse_mappings = {}\n",
        "    for node_type, mapping in mappings.items():\n",
        "        reverse_mappings[node_type] = {v: k for k, v in mapping.items()}\n",
        "\n",
        "    train_set = set(zip(train_df['user_id'], train_df['movie_id']))\n",
        "    val_set = set(zip(val_df['user_id'], val_df['movie_id']))\n",
        "    test_set = set(zip(test_df['user_id'], test_df['movie_id']))\n",
        "    if ('user', 'rates', 'movie') in graph.edge_types:\n",
        "        edge_index = graph['user', 'rates', 'movie'].edge_index\n",
        "        num_edges = edge_index.shape[1]\n",
        "        train_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
        "        val_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
        "        test_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
        "\n",
        "        for i in range(num_edges):\n",
        "            user_idx = edge_index[0, i].item()\n",
        "            movie_idx = edge_index[1, i].item()\n",
        "            user_id = reverse_mappings['user'].get(user_idx)\n",
        "            movie_id = reverse_mappings['movie'].get(movie_idx)\n",
        "\n",
        "            if user_id is not None and movie_id is not None:\n",
        "                edge_tuple = (user_id, movie_id)\n",
        "                if edge_tuple in train_set:\n",
        "                    train_mask[i] = True\n",
        "                elif edge_tuple in val_set:\n",
        "                    val_mask[i] = True\n",
        "                elif edge_tuple in test_set:\n",
        "                    test_mask[i] = True\n",
        "\n",
        "        graph['user', 'rates', 'movie'].train_mask = train_mask\n",
        "        graph['user', 'rates', 'movie'].val_mask = val_mask\n",
        "        graph['user', 'rates', 'movie'].test_mask = test_mask\n",
        "\n",
        "        if ('movie', 'rev_rates', 'user') in graph.edge_types:\n",
        "            graph['movie', 'rev_rates', 'user'].train_mask = train_mask\n",
        "            graph['movie', 'rev_rates', 'user'].val_mask = val_mask\n",
        "            graph['movie', 'rev_rates', 'user'].test_mask = test_mask\n",
        "\n",
        "    return graph\n"
      ],
      "metadata": {
        "id": "G3TYMzkYYA_o"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Graph Preparation**\n",
        "\n",
        "Performs a complete pipeline: splits user-movie interactions into train/val/test sets, assigns corresponding masks to graph edges, and precomputes negative sampling candidates for each user by excluding their rated movies."
      ],
      "metadata": {
        "id": "3bQOPnLTBq0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_graph_with_splits(subgraph, mappings, user_movie_edges, num_negatives=200):\n",
        "    train_df, val_df, test_df = split_data_stratified(\n",
        "        user_movie_edges,\n",
        "        train_ratio=0.7,\n",
        "        val_ratio=0.15,\n",
        "        test_ratio=0.15\n",
        "    )\n",
        "\n",
        "    reverse_mappings = {}\n",
        "\n",
        "    for node_type, mapping in mappings.items():\n",
        "        reverse_mappings[node_type] = {v: k for k, v in mapping.items()}\n",
        "\n",
        "    graph = add_split_masks_fast(subgraph, train_df, val_df, test_df, mappings)\n",
        "    user_train_movies = {}\n",
        "\n",
        "    for _, row in train_df.iterrows():\n",
        "        user_id = row['user_id']\n",
        "        movie_id = row['movie_id']\n",
        "\n",
        "        if user_id not in user_train_movies:\n",
        "            user_train_movies[user_id] = set()\n",
        "        user_train_movies[user_id].add(movie_id)\n",
        "\n",
        "    all_movie_ids = set(mappings['movie'].keys())\n",
        "    negative_candidates = {}\n",
        "\n",
        "    for user_id, rated_movies in user_train_movies.items():\n",
        "        candidate_movies = all_movie_ids - rated_movies\n",
        "        negative_candidates[user_id] = list(candidate_movies)\n",
        "\n",
        "    splits = {\n",
        "        'train_df': train_df,\n",
        "        'val_df': val_df,\n",
        "        'test_df': test_df,\n",
        "        'user_train_movies': user_train_movies,\n",
        "        'negative_candidates': negative_candidates,\n",
        "        'all_movie_ids': all_movie_ids,\n",
        "        'reverse_mappings': reverse_mappings\n",
        "    }\n",
        "\n",
        "    print(f'   Graph contains {graph.num_edges} edges')\n",
        "    print(f'   {len(negative_candidates)} users available for negative sampling')\n",
        "\n",
        "    return graph, splits\n"
      ],
      "metadata": {
        "id": "letjQtTRYLSy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Simple Train/Val/Test Split**\n",
        "\n",
        "Performs a user-level random split of interactions without stratifying by rating. Ensures each user has at least one interaction in the training set."
      ],
      "metadata": {
        "id": "x3w10_MICE6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data_simple(df_user_movie, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    train_dfs, val_dfs, test_dfs = [], [], []\n",
        "\n",
        "    for user_id, group in df_user_movie.groupby('user_id'):\n",
        "        n = len(group)\n",
        "        if n == 0:\n",
        "            continue\n",
        "\n",
        "        n_test = max(1, int(n * test_ratio))\n",
        "        n_val = max(1, int(n * val_ratio))\n",
        "        n_train = n - n_val - n_test\n",
        "\n",
        "        if n_train <= 0:\n",
        "            n_train = 1\n",
        "            n_val = min(n - 1, n_val)\n",
        "            n_test = n - n_train - n_val\n",
        "\n",
        "        indices = np.random.permutation(n)\n",
        "\n",
        "        train_idx = indices[:n_train]\n",
        "        val_idx = indices[n_train:n_train + n_val]\n",
        "        test_idx = indices[n_train + n_val:]\n",
        "\n",
        "        train_dfs.append(group.iloc[train_idx])\n",
        "        val_dfs.append(group.iloc[val_idx])\n",
        "        test_dfs.append(group.iloc[test_idx])\n",
        "\n",
        "    train_df = pd.concat(train_dfs, ignore_index=True)\n",
        "    val_df = pd.concat(val_dfs, ignore_index=True)\n",
        "    test_df = pd.concat(test_dfs, ignore_index=True)\n",
        "\n",
        "    print(f'Train: {len(train_df)} links ({len(train_df) / len(df_user_movie) * 100:.1f}%)')\n",
        "    print(f'Val: {len(val_df)} links ({len(val_df) / len(df_user_movie) * 100:.1f}%)')\n",
        "    print(f'Test: {len(test_df)} links ({len(test_df) / len(df_user_movie) * 100:.1f}%)')\n",
        "\n",
        "    return train_df, val_df, test_df\n"
      ],
      "metadata": {
        "id": "TxC-UL7peHm1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "active_user_ids = get_most_active_users(\n",
        "    df_connections_user_movie,\n",
        "    df_connections_user_stats,\n",
        "    n_users=5000\n",
        ")\n",
        "\n",
        "subgraph, mappings, user_movie_edges = build_subgraph_for_users(\n",
        "    user_ids=active_user_ids,\n",
        "    df_user_movie=df_connections_user_movie,\n",
        "    df_movie_features=df_connections_movie_features,\n",
        "    df_user_stats=df_connections_user_stats,\n",
        "    df_movie_genre=df_connections_movie_genre,\n",
        "    df_movie_actor=df_connections_movie_actor,\n",
        "    df_movie_director=df_connections_movie_director,\n",
        "    df_actors=df_connections_actors,\n",
        "    df_directors=df_connections_directors,\n",
        "    df_genres=df_connections_genres,\n",
        "    num_negatives=500\n",
        ")\n",
        "\n",
        "train_df, val_df, test_df = split_data_simple(\n",
        "    user_movie_edges,\n",
        "    train_ratio=0.7,\n",
        "    val_ratio=0.15,\n",
        "    test_ratio=0.15\n",
        ")\n",
        "\n",
        "reverse_mappings = {}\n",
        "for node_type, mapping in mappings.items():\n",
        "    reverse_mappings[node_type] = {v: k for k, v in mapping.items()}\n",
        "\n",
        "train_set = set(zip(train_df['user_id'], train_df['movie_id']))\n",
        "val_set = set(zip(val_df['user_id'], val_df['movie_id']))\n",
        "test_set = set(zip(test_df['user_id'], test_df['movie_id']))\n",
        "\n",
        "if ('user', 'rates', 'movie') in subgraph.edge_types:\n",
        "    edge_index = subgraph['user', 'rates', 'movie'].edge_index\n",
        "    num_edges = edge_index.shape[1]\n",
        "    train_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
        "    val_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
        "\n",
        "    for i in range(num_edges):\n",
        "        user_idx = edge_index[0, i].item()\n",
        "        movie_idx = edge_index[1, i].item()\n",
        "        user_id = reverse_mappings['user'].get(user_idx)\n",
        "        movie_id = reverse_mappings['movie'].get(movie_idx)\n",
        "\n",
        "        if user_id is not None and movie_id is not None:\n",
        "            edge_tuple = (user_id, movie_id)\n",
        "            if edge_tuple in train_set:\n",
        "                train_mask[i] = True\n",
        "            elif edge_tuple in val_set:\n",
        "                val_mask[i] = True\n",
        "            elif edge_tuple in test_set:\n",
        "                test_mask[i] = True\n",
        "    subgraph['user', 'rates', 'movie'].train_mask = train_mask\n",
        "    subgraph['user', 'rates', 'movie'].val_mask = val_mask\n",
        "    subgraph['user', 'rates', 'movie'].test_mask = test_mask\n",
        "\n",
        "    if ('movie', 'rev_rates', 'user') in subgraph.edge_types:\n",
        "        subgraph['movie', 'rev_rates', 'user'].train_mask = train_mask\n",
        "        subgraph['movie', 'rev_rates', 'user'].val_mask = val_mask\n",
        "        subgraph['movie', 'rev_rates', 'user'].test_mask = test_mask\n",
        "\n",
        "user_train_movies = {}\n",
        "\n",
        "for _, row in train_df.iterrows():\n",
        "    user_id = row['user_id']\n",
        "    movie_id = row['movie_id']\n",
        "\n",
        "    if user_id not in user_train_movies:\n",
        "        user_train_movies[user_id] = set()\n",
        "    user_train_movies[user_id].add(movie_id)\n",
        "\n",
        "all_movie_ids = set(mappings['movie'].keys())\n",
        "negative_candidates = {}\n",
        "\n",
        "for user_id in active_user_ids:\n",
        "    if user_id in user_train_movies:\n",
        "        rated_movies = user_train_movies[user_id]\n",
        "        candidate_movies = all_movie_ids - rated_movies\n",
        "        negative_candidates[user_id] = list(candidate_movies)\n",
        "    else:\n",
        "        negative_candidates[user_id] = list(all_movie_ids)\n",
        "\n",
        "splits_info = {\n",
        "    'train_df': train_df,\n",
        "    'val_df': val_df,\n",
        "    'test_df': test_df,\n",
        "    'user_train_movies': user_train_movies,\n",
        "    'negative_candidates': negative_candidates,\n",
        "    'all_movie_ids': all_movie_ids,\n",
        "    'reverse_mappings': reverse_mappings,\n",
        "    'active_user_ids': active_user_ids\n",
        "}\n",
        "\n",
        "torch.save({\n",
        "    'graph': subgraph,\n",
        "    'splits': splits_info,\n",
        "    'mappings': mappings\n",
        "}, 'prepared_graph.pt')\n",
        "\n",
        "print(f'Saved to file: \\'prepared_graph.pt\\'')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVikjSJodmR4",
        "outputId": "40a46246-a5a5-498b-92be-24ddbc3ffbb0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 5000 most active users\n",
            "Minimum number of ratings: 52\n",
            "Maximum number of ratings: 56\n",
            "Average number of ratings: 54.28\n",
            "\n",
            "Sizes of filtered data:\n",
            "Users: 5000\n",
            "Movies: 8579\n",
            "User-movie links: 271376\n",
            "Actors: 2512\n",
            "Directors: 1600\n",
            "Genres: 20\n",
            "\n",
            "Constructed graph with the following structure:\n",
            "Nodes: ['user', 'movie', 'actor', 'director', 'genre']\n",
            "Edges: [('user', 'rates', 'movie'), ('movie', 'has_genre', 'genre'), ('movie', 'has_director', 'director'), ('movie', 'has_actor', 'actor'), ('genre', 'rev_has_genre', 'movie'), ('director', 'rev_has_director', 'movie'), ('actor', 'rev_has_actor', 'movie'), ('movie', 'rev_rates', 'user')]\n",
            "\n",
            "Checking node features:\n",
            "  user: torch.Size([5000, 3])\n",
            "  movie: torch.Size([8579, 3])\n",
            "  actor: torch.Size([2512, 5])\n",
            "  director: torch.Size([1600, 4])\n",
            "  genre: torch.Size([20, 20])\n",
            "\n",
            "Checking edges:\n",
            "  ('user', 'rates', 'movie'): 271376 edges\n",
            "  ('movie', 'has_genre', 'genre'): 21323 edges\n",
            "  ('movie', 'has_director', 'director'): 6605 edges\n",
            "  ('movie', 'has_actor', 'actor'): 32402 edges\n",
            "  ('genre', 'rev_has_genre', 'movie'): 21323 edges\n",
            "  ('director', 'rev_has_director', 'movie'): 6605 edges\n",
            "  ('actor', 'rev_has_actor', 'movie'): 32402 edges\n",
            "  ('movie', 'rev_rates', 'user'): 271376 edges\n",
            "Train: 194520 links (71.7%)\n",
            "Val: 38428 links (14.2%)\n",
            "Test: 38428 links (14.2%)\n",
            "Saved to file: 'prepared_graph.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Recommendation Dataset**\n",
        "\n",
        "A dataset class that stores negative candidates as pre-mapped indices for faster batch generation."
      ],
      "metadata": {
        "id": "eniD5J_QCUNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RecDataset(Dataset):\n",
        "    def __init__(self, df, negative_candidates, mappings, num_negatives=10, rng_seed: int = None):\n",
        "        self.df = df\n",
        "        self.negative_candidates = negative_candidates\n",
        "        self.mappings = mappings\n",
        "        self.num_negatives = num_negatives\n",
        "\n",
        "        self.rng = np.random.default_rng(rng_seed)\n",
        "\n",
        "        self.data = []\n",
        "        for _, row in df.iterrows():\n",
        "            user_idx = mappings['user'][row['user_id']]\n",
        "            movie_idx = mappings['movie'][row['movie_id']]\n",
        "            self.data.append({\n",
        "                'user_idx': int(user_idx),\n",
        "                'movie_idx': int(movie_idx),\n",
        "                'rating': float(row['rating'])\n",
        "            })\n",
        "\n",
        "        self.neg_candidates_idx = {}\n",
        "        for user_id, movie_ids in negative_candidates.items():\n",
        "            user_idx = mappings['user'][user_id]\n",
        "            self.neg_candidates_idx[int(user_idx)] = [\n",
        "                int(mappings['movie'][mid]) for mid in movie_ids\n",
        "            ]\n",
        "\n",
        "        self._all_movie_indices = list(mappings['movie'].values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        user_idx = item['user_idx']\n",
        "\n",
        "        if user_idx in self.neg_candidates_idx and len(self.neg_candidates_idx[user_idx]) > 0:\n",
        "            candidates = self.neg_candidates_idx[user_idx]\n",
        "            if len(candidates) >= self.num_negatives:\n",
        "                neg_indices = self.rng.choice(candidates, self.num_negatives, replace=False)\n",
        "            else:\n",
        "                neg_indices = self.rng.choice(candidates, self.num_negatives, replace=True)\n",
        "        else:\n",
        "            if len(self._all_movie_indices) >= self.num_negatives:\n",
        "                neg_indices = self.rng.choice(self._all_movie_indices, self.num_negatives, replace=False)\n",
        "            else:\n",
        "                neg_indices = self.rng.choice(self._all_movie_indices, self.num_negatives, replace=True)\n",
        "\n",
        "        return {\n",
        "            'user_idx': torch.tensor(user_idx, dtype=torch.long),\n",
        "            'pos_movie_idx': torch.tensor(item['movie_idx'], dtype=torch.long),\n",
        "            'neg_movie_indices': torch.tensor(neg_indices, dtype=torch.long),\n",
        "            'rating': torch.tensor(item['rating'], dtype=torch.float)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "PeghqLWGGIOR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Heterogeneous GraphConv Model**\n",
        "\n",
        "A multi-layer heterogeneous graph neural network that replaces SAGEConv with standard GraphConv layers. The architecture supports variable number of layers, with separate linear projections for each node type and final user/movie embedding refinement layers."
      ],
      "metadata": {
        "id": "Pf7SAkv8CkLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HeteroGraphConv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 user_feat_dim,\n",
        "                 movie_feat_dim,\n",
        "                 actor_feat_dim,\n",
        "                 director_feat_dim,\n",
        "                 genre_feat_dim,\n",
        "                 hidden_channels=64,\n",
        "                 out_channels=32,\n",
        "                 num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.user_lin = Linear(user_feat_dim, hidden_channels)\n",
        "        self.movie_lin = Linear(movie_feat_dim, hidden_channels)\n",
        "        self.actor_lin = Linear(actor_feat_dim, hidden_channels)\n",
        "        self.director_lin = Linear(director_feat_dim, hidden_channels)\n",
        "        self.genre_lin = Linear(genre_feat_dim, hidden_channels)\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            in_channels = hidden_channels\n",
        "            out_channels_layer = out_channels if i == num_layers - 1 else hidden_channels\n",
        "\n",
        "            conv = HeteroConv({\n",
        "                ('user', 'rates', 'movie'): GraphConv(in_channels, out_channels_layer),\n",
        "                ('movie', 'rev_rates', 'user'): GraphConv(in_channels, out_channels_layer),\n",
        "\n",
        "                ('movie', 'has_genre', 'genre'): GraphConv(in_channels, out_channels_layer),\n",
        "                ('genre', 'rev_has_genre', 'movie'): GraphConv(in_channels, out_channels_layer),\n",
        "\n",
        "                ('movie', 'has_director', 'director'): GraphConv(in_channels, out_channels_layer),\n",
        "                ('director', 'rev_has_director', 'movie'): GraphConv(in_channels, out_channels_layer),\n",
        "\n",
        "                ('movie', 'has_actor', 'actor'): GraphConv(in_channels, out_channels_layer),\n",
        "                ('actor', 'rev_has_actor', 'movie'): GraphConv(in_channels, out_channels_layer),\n",
        "            }, aggr='sum')\n",
        "\n",
        "            self.convs.append(conv)\n",
        "\n",
        "        self.user_final = Linear(out_channels, out_channels)\n",
        "        self.movie_final = Linear(out_channels, out_channels)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, (nn.Linear, GraphConv)):\n",
        "                module.reset_parameters()\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        x_dict = {\n",
        "            'user': self.user_lin(x_dict['user']),\n",
        "            'movie': self.movie_lin(x_dict['movie']),\n",
        "            'actor': self.actor_lin(x_dict['actor']),\n",
        "            'director': self.director_lin(x_dict['director']),\n",
        "            'genre': self.genre_lin(x_dict['genre']),\n",
        "        }\n",
        "\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            x_dict = conv(x_dict, edge_index_dict)\n",
        "\n",
        "            x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
        "\n",
        "        user_emb = self.user_final(x_dict['user'])\n",
        "        movie_emb = self.movie_final(x_dict['movie'])\n",
        "\n",
        "        return user_emb, movie_emb\n"
      ],
      "metadata": {
        "id": "8rEbSpVdzDmO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Pipeline**\n",
        "\n",
        "### **Core Components**\n",
        "\n",
        "**Model Architecture:**\n",
        "- **HeteroGraphConv**: Multi-layer heterogeneous GNN with GraphConv layers\n",
        "- **Feature Projection**: Separate linear layers for each node type (user, movie, actor, director, genre)\n",
        "- **Mask-Aware Embeddings**: Uses train/val edge masks during message passing\n",
        "- **Final Embeddings**: Additional linear layers refine user and movie embeddings\n",
        "\n",
        "\n",
        "**Training:**\n",
        "- **Loss Function**: BCEWithLogitsLoss comparing positive vs negative movie scores\n",
        "\n",
        "**Evaluation:**\n",
        "- **Comprehensive Metrics**:\n",
        "  - Classification: Accuracy, Precision, Recall, F1, AUC-ROC\n",
        "  - Ranking: Precision@K, Recall@K, F1@K, NDCG@K, MAP@K (for K=5,10,20)"
      ],
      "metadata": {
        "id": "aEMWy1sAC8wW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List, Dict\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "class RecDataset(Dataset):\n",
        "    def __init__(self, df, negative_candidates, mappings, num_negatives=10, rng_seed=None):\n",
        "        self.df = df\n",
        "        self.negative_candidates = negative_candidates\n",
        "        self.mappings = mappings\n",
        "        self.num_negatives = num_negatives\n",
        "        self.rng = np.random.default_rng(rng_seed)\n",
        "        self.data = []\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            user_idx = mappings['user'][row['user_id']]\n",
        "            movie_idx = mappings['movie'][row['movie_id']]\n",
        "            self.data.append({\n",
        "                'user_idx': int(user_idx),\n",
        "                'movie_idx': int(movie_idx),\n",
        "                'rating': float(row['rating'])\n",
        "            })\n",
        "\n",
        "        self.neg_candidates_idx = {}\n",
        "        for user_id, movie_ids in negative_candidates.items():\n",
        "            user_idx = mappings['user'][user_id]\n",
        "            self.neg_candidates_idx[int(user_idx)] = [\n",
        "                int(mappings['movie'][mid]) for mid in movie_ids\n",
        "            ]\n",
        "\n",
        "        self._all_movie_indices = list(mappings['movie'].values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        user_idx = item['user_idx']\n",
        "\n",
        "        if user_idx in self.neg_candidates_idx and len(self.neg_candidates_idx[user_idx]) > 0:\n",
        "            candidates = self.neg_candidates_idx[user_idx]\n",
        "            if len(candidates) >= self.num_negatives:\n",
        "                neg_indices = self.rng.choice(candidates, self.num_negatives, replace=False)\n",
        "            else:\n",
        "                neg_indices = self.rng.choice(candidates, self.num_negatives, replace=True)\n",
        "        else:\n",
        "            if len(self._all_movie_indices) >= self.num_negatives:\n",
        "                neg_indices = self.rng.choice(self._all_movie_indices, self.num_negatives, replace=False)\n",
        "            else:\n",
        "                neg_indices = self.rng.choice(self._all_movie_indices, self.num_negatives, replace=True)\n",
        "\n",
        "        return {\n",
        "            'user_idx': torch.tensor(user_idx, dtype=torch.long),\n",
        "            'pos_movie_idx': torch.tensor(item['movie_idx'], dtype=torch.long),\n",
        "            'neg_movie_indices': torch.tensor(neg_indices, dtype=torch.long),\n",
        "            'rating': torch.tensor(item['rating'], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "def dict_collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
        "    collated = {}\n",
        "    if len(batch) == 0:\n",
        "        return collated\n",
        "    keys = batch[0].keys()\n",
        "    for k in keys:\n",
        "        collated[k] = torch.stack([d[k] for d in batch], dim=0)\n",
        "    return collated\n",
        "\n",
        "def create_dataloaders(splits, mappings, batch_size=32, num_negatives=10, num_workers=2, pin_memory: bool = True):\n",
        "    train_dataset = RecDataset(splits['train_df'], splits['negative_candidates'], mappings, num_negatives)\n",
        "    val_dataset = RecDataset(splits['val_df'], splits['negative_candidates'], mappings, num_negatives)\n",
        "    test_dataset = RecDataset(splits['test_df'], splits['negative_candidates'], mappings, num_negatives)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True,\n",
        "        collate_fn=dict_collate_fn, num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False,\n",
        "        collate_fn=dict_collate_fn, num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False,\n",
        "        collate_fn=dict_collate_fn, num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def _model_device(model: torch.nn.Module) -> torch.device:\n",
        "    try:\n",
        "        return next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def get_embeddings_with_masks(model, graph, use_train_mask=True):\n",
        "    device = _model_device(model)\n",
        "    x_dict_on_device = {}\n",
        "    for k, v in graph.x_dict.items():\n",
        "        x_dict_on_device[k] = v.to(device)\n",
        "    edge_index_dict = {}\n",
        "    for edge_type in graph.edge_types:\n",
        "        data_edge = graph[edge_type]\n",
        "        if use_train_mask and hasattr(data_edge, 'train_mask'):\n",
        "            edge_index = data_edge.edge_index[:, data_edge.train_mask]\n",
        "            edge_index_dict[edge_type] = edge_index.to(device)\n",
        "        else:\n",
        "            edge_index_dict[edge_type] = data_edge.edge_index.to(device)\n",
        "    user_emb, movie_emb = model(x_dict_on_device, edge_index_dict)\n",
        "    user_emb = user_emb.to(device)\n",
        "    movie_emb = movie_emb.to(device)\n",
        "    return user_emb, movie_emb\n",
        "\n",
        "def batch_loss(model, graph, batch, use_train_mask=True):\n",
        "    user_emb, movie_emb = get_embeddings_with_masks(model, graph, use_train_mask)\n",
        "    user_emb_batch = user_emb[batch['user_idx']]\n",
        "    pos_movie_emb = movie_emb[batch['pos_movie_idx']]\n",
        "    neg_movie_emb = movie_emb[batch['neg_movie_indices']]\n",
        "\n",
        "    pos_scores = torch.sum(user_emb_batch * pos_movie_emb, dim=1)\n",
        "    neg_scores = torch.sum(user_emb_batch.unsqueeze(1) * neg_movie_emb, dim=2)\n",
        "\n",
        "    pos_targets = torch.ones_like(pos_scores)\n",
        "    neg_targets = torch.zeros_like(neg_scores)\n",
        "\n",
        "    all_scores = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)\n",
        "    all_targets = torch.cat([pos_targets.unsqueeze(1), neg_targets], dim=1)\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "    return loss_fn(all_scores, all_targets)\n",
        "\n",
        "def simple_train_epoch(model, graph, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        loss = batch_loss(model, graph, batch, use_train_mask=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    return total_loss / max(1, num_batches)\n",
        "\n",
        "def validate(model, graph, val_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
        "            loss = batch_loss(model, graph, batch, use_train_mask=False)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "    return total_loss / max(1, num_batches)\n",
        "\n",
        "def compute_ranking_metrics(\n",
        "        model,\n",
        "        graph,\n",
        "        splits,\n",
        "        mappings,\n",
        "        k_list: List[int] = [5, 10, 20],\n",
        "        max_users: int = 200,\n",
        "        max_negatives_per_user: int = 100,\n",
        "        positive_threshold: float = 4.0\n",
        "):\n",
        "    model.eval()\n",
        "    device = _model_device(model)\n",
        "    user_emb, movie_emb = get_embeddings_with_masks(model, graph, use_train_mask=False)\n",
        "    val_users = splits['val_df']['user_id'].unique()\n",
        "    if len(val_users) > max_users:\n",
        "        val_users = np.random.choice(val_users, max_users, replace=False)\n",
        "    metrics_accum = {f'precision@{k}': [] for k in k_list}\n",
        "    metrics_accum.update({f'recall@{k}': [] for k in k_list})\n",
        "    metrics_accum.update({f'f1@{k}': [] for k in k_list})\n",
        "    metrics_accum.update({f'ndcg@{k}': [] for k in k_list})\n",
        "    metrics_accum.update({f'map@{k}': [] for k in k_list})\n",
        "    all_true = []\n",
        "    all_pred = []\n",
        "    all_pred_proba = []\n",
        "    with torch.no_grad():\n",
        "        for user_id in val_users:\n",
        "            if user_id not in mappings['user']:\n",
        "                continue\n",
        "            user_idx = mappings['user'][user_id]\n",
        "            user_val_rows = splits['val_df'][splits['val_df']['user_id'] == user_id]\n",
        "            user_val_positives = user_val_rows[user_val_rows['rating'] >= positive_threshold]['movie_id'].tolist()\n",
        "            if len(user_val_positives) == 0:\n",
        "                continue\n",
        "            if user_id in splits['negative_candidates']:\n",
        "                neg_cands = splits['negative_candidates'][user_id]\n",
        "                if len(neg_cands) > max_negatives_per_user:\n",
        "                    neg_cands = list(np.random.choice(neg_cands, max_negatives_per_user, replace=False))\n",
        "            else:\n",
        "                neg_cands = [mid for mid in splits['all_movie_ids'] if mid not in splits['user_train_movies'].get(user_id, set())]\n",
        "                if len(neg_cands) > max_negatives_per_user:\n",
        "                    neg_cands = list(np.random.choice(neg_cands, max_negatives_per_user, replace=False))\n",
        "            if len(neg_cands) == 0:\n",
        "                continue\n",
        "            candidate_movies = list(neg_cands) + list(user_val_positives)\n",
        "            candidate_movies = [mid for mid in candidate_movies if mid in mappings['movie']]\n",
        "            if len(candidate_movies) == 0:\n",
        "                continue\n",
        "            candidate_indices = [mappings['movie'][mid] for mid in candidate_movies]\n",
        "            user_emb_single = user_emb[user_idx:user_idx + 1]\n",
        "            candidate_emb = movie_emb[candidate_indices]\n",
        "            scores = torch.matmul(user_emb_single, candidate_emb.T).squeeze(0)\n",
        "            probs = torch.sigmoid(scores).cpu().numpy()\n",
        "            relevance = np.array([1 if mid in user_val_positives else 0 for mid in candidate_movies], dtype=int)\n",
        "            binary_preds = (probs > 0.5).astype(int)\n",
        "            all_true.extend(relevance.tolist())\n",
        "            all_pred.extend(binary_preds.tolist())\n",
        "            all_pred_proba.extend(probs.tolist())\n",
        "            sorted_idx = np.argsort(-scores.cpu().numpy())\n",
        "            sorted_relevance = relevance[sorted_idx]\n",
        "            sorted_scores_for_ndcg = scores.cpu().numpy()[sorted_idx]\n",
        "            for k in k_list:\n",
        "                if len(sorted_relevance) < 1:\n",
        "                    continue\n",
        "                topk = sorted_relevance[:k]\n",
        "                precision_at_k = topk.sum() / k\n",
        "                recall_at_k = topk.sum() / max(1, relevance.sum())\n",
        "                if precision_at_k + recall_at_k > 0:\n",
        "                    f1_at_k = 2 * precision_at_k * recall_at_k / (precision_at_k + recall_at_k)\n",
        "                else:\n",
        "                    f1_at_k = 0.0\n",
        "                metrics_accum[f'precision@{k}'].append(precision_at_k)\n",
        "                metrics_accum[f'recall@{k}'].append(recall_at_k)\n",
        "                metrics_accum[f'f1@{k}'].append(f1_at_k)\n",
        "                try:\n",
        "                    ndcg_at_k = ndcg_score([relevance], [scores.cpu().numpy()], k=k)\n",
        "                except Exception:\n",
        "                    ndcg_at_k = 0.0\n",
        "                metrics_accum[f'ndcg@{k}'].append(ndcg_at_k)\n",
        "                map_at_k = 0.0\n",
        "                num_relevant = 0\n",
        "                for i in range(min(k, len(sorted_relevance))):\n",
        "                    if sorted_relevance[i] == 1:\n",
        "                        num_relevant += 1\n",
        "                        map_at_k += num_relevant / (i + 1)\n",
        "                denom = min(relevance.sum(), k) if relevance.sum() > 0 else 1\n",
        "                map_at_k = map_at_k / denom\n",
        "                metrics_accum[f'map@{k}'].append(map_at_k)\n",
        "    avg_metrics = {}\n",
        "    for k in k_list:\n",
        "        for mname in ['precision', 'recall', 'f1', 'ndcg', 'map']:\n",
        "            key = f'{mname}@{k}'\n",
        "            vals = metrics_accum.get(key, [])\n",
        "            avg_metrics[key] = float(np.mean(vals)) if len(vals) > 0 else 0.0\n",
        "    if len(all_true) > 0:\n",
        "        avg_metrics['accuracy'] = float(accuracy_score(all_true, all_pred))\n",
        "        avg_metrics['precision'] = float(precision_score(all_true, all_pred, zero_division=0))\n",
        "        avg_metrics['recall'] = float(recall_score(all_true, all_pred, zero_division=0))\n",
        "        avg_metrics['f1'] = float(f1_score(all_true, all_pred, zero_division=0))\n",
        "        try:\n",
        "            avg_metrics['auc_roc'] = float(roc_auc_score(all_true, all_pred_proba))\n",
        "        except Exception:\n",
        "            avg_metrics['auc_roc'] = 0.0\n",
        "    else:\n",
        "        avg_metrics.update({\n",
        "            'accuracy': 0.0,\n",
        "            'precision': 0.0,\n",
        "            'recall': 0.0,\n",
        "            'f1': 0.0,\n",
        "            'auc_roc': 0.0\n",
        "        })\n",
        "    return avg_metrics\n",
        "\n",
        "def evaluate(model, graph, splits, mappings, k_list=[5, 10, 20], max_users=500, max_negatives_per_user=100):\n",
        "    metrics = compute_ranking_metrics(model, graph, splits, mappings, k_list=k_list, max_users=max_users, max_negatives_per_user=max_negatives_per_user)\n",
        "    print('\\nClassification metrics:')\n",
        "    print(f'Accuracy:  {metrics[\"accuracy\"]:.4f}')\n",
        "    print(f'Precision: {metrics[\"precision\"]:.4f}')\n",
        "    print(f'Recall:    {metrics[\"recall\"]:.4f}')\n",
        "    print(f'F1-score:  {metrics[\"f1\"]:.4f}')\n",
        "    print(f'AUC-ROC:   {metrics[\"auc_roc\"]:.4f}')\n",
        "    print('\\nRanking metrics:')\n",
        "    for k in k_list:\n",
        "        print(f\"@k={k}: Precision: {metrics[f'precision@{k}']:.4f}, Recall: {metrics[f'recall@{k}']:.4f}\")\n",
        "        print(f\"F1: {metrics[f'f1@{k}']:.4f}, NDCG: {metrics[f'ndcg@{k}']:.4f}, MAP: {metrics[f'map@{k}']:.4f}\")\n",
        "    return metrics\n",
        "\n",
        "def train(prepared_path='prepared_graph.pt', epochs=5, batch_size=32, eval_every=2, k_list=[5, 10, 20], hidden_channels=64, out_channels=32):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    use_pin_memory = device.type == 'cuda'\n",
        "    print(f'Device: {device}, pin_memory={use_pin_memory}')\n",
        "    loaded_data = torch.load(prepared_path, map_location='cpu', weights_only=False)\n",
        "    graph = loaded_data['graph']\n",
        "    splits = loaded_data['splits']\n",
        "    mappings = loaded_data['mappings']\n",
        "    user_feat_dim = graph['user'].x.shape[1]\n",
        "    movie_feat_dim = graph['movie'].x.shape[1]\n",
        "    actor_feat_dim = graph['actor'].x.shape[1]\n",
        "    director_feat_dim = graph['director'].x.shape[1]\n",
        "    genre_feat_dim = graph['genre'].x.shape[1]\n",
        "    model = HeteroGraphConv(\n",
        "        user_feat_dim=user_feat_dim,\n",
        "        movie_feat_dim=movie_feat_dim,\n",
        "        actor_feat_dim=actor_feat_dim,\n",
        "        director_feat_dim=director_feat_dim,\n",
        "        genre_feat_dim=genre_feat_dim,\n",
        "        hidden_channels=hidden_channels,\n",
        "        out_channels=out_channels,\n",
        "        num_layers=2\n",
        "    )\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(\n",
        "        splits, mappings, batch_size=batch_size, num_negatives=10,\n",
        "        pin_memory=use_pin_memory\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    try:\n",
        "        graph = graph.to(device)\n",
        "    except Exception:\n",
        "        print('graph.to(device) not available')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    history = {'train_loss': [], 'val_loss': [], 'metrics': []}\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss = simple_train_epoch(model, graph, train_loader, optimizer, device)\n",
        "        val_loss = validate(model, graph, val_loader, device)\n",
        "        if (epoch + 1) % eval_every == 0 or epoch == epochs - 1:\n",
        "            print(f'\\nEvaluating metrics at epoch {epoch + 1}...')\n",
        "            metrics = evaluate(\n",
        "                model,\n",
        "                graph,\n",
        "                splits,\n",
        "                mappings,\n",
        "                k_list=k_list,\n",
        "                max_users=500,\n",
        "                max_negatives_per_user=200)\n",
        "            history['metrics'].append(metrics)\n",
        "        else:\n",
        "            metrics = None\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        if metrics:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "            print(f\"Sample: Precision@10: {metrics.get('precision@10', 0):.4f}, NDCG@10: {metrics.get('ndcg@10', 0):.4f}\")\n",
        "        else:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "    print('\\nFinal evaluation on TEST set...')\n",
        "    final_metrics = compute_ranking_metrics(\n",
        "        model, graph,\n",
        "        {**splits, 'val_df': splits['test_df']},\n",
        "        mappings,\n",
        "        k_list=k_list,\n",
        "        max_users=min(500, len(splits['test_df']['user_id'].unique())),\n",
        "        max_negatives_per_user=200\n",
        "    )\n",
        "    print('Final test metrics (subset):')\n",
        "    print(f\"  Precision@10: {final_metrics.get('precision@10', 0):.4f}\")\n",
        "    print(f\"  Recall@10:    {final_metrics.get('recall@10', 0):.4f}\")\n",
        "    print(f\"  NDCG@10:      {final_metrics.get('ndcg@10', 0):.4f}\")\n",
        "    print(f\"  AUC-ROC:      {final_metrics.get('auc_roc', 0):.4f}\")\n",
        "    return model, history, final_metrics\n",
        "\n",
        "model, history, final_metrics = train(\n",
        "    prepared_path='prepared_graph.pt',\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    eval_every=2,\n",
        "    k_list=[5, 10, 20],\n",
        "    hidden_channels=32,\n",
        "    out_channels=16\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI1d2X6GT5P9",
        "outputId": "d734cac2-3c4e-42fa-d740-ea40a1d9bd95"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda, pin_memory=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [03:46<15:05, 226.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5: Train Loss: 75.0007, Val Loss: 1.8791\n",
            "\n",
            "Evaluating metrics at epoch 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [07:55<11:59, 239.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification metrics:\n",
            "Accuracy:  0.9772\n",
            "Precision: 0.0000\n",
            "Recall:    0.0000\n",
            "F1-score:  0.0000\n",
            "AUC-ROC:   0.4950\n",
            "\n",
            "Ranking metrics:\n",
            "@k=5: Precision: 0.0004, Recall: 0.0003\n",
            "F1: 0.0004, NDCG: 0.0253, MAP: 0.0004\n",
            "@k=10: Precision: 0.0004, Recall: 0.0006\n",
            "F1: 0.0005, NDCG: 0.0361, MAP: 0.0004\n",
            "@k=20: Precision: 0.0003, Recall: 0.0009\n",
            "F1: 0.0005, NDCG: 0.0559, MAP: 0.0004\n",
            "Epoch 2/5: Train Loss: 0.4054, Val Loss: 0.3393\n",
            "Sample: Precision@10: 0.0004, NDCG@10: 0.0361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [11:03<07:12, 216.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5: Train Loss: 0.3055, Val Loss: 0.3180\n",
            "\n",
            "Evaluating metrics at epoch 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [14:14<03:26, 206.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification metrics:\n",
            "Accuracy:  0.9770\n",
            "Precision: 0.0000\n",
            "Recall:    0.0000\n",
            "F1-score:  0.0000\n",
            "AUC-ROC:   0.5035\n",
            "\n",
            "Ranking metrics:\n",
            "@k=5: Precision: 0.0004, Recall: 0.0003\n",
            "F1: 0.0003, NDCG: 0.0254, MAP: 0.0001\n",
            "@k=10: Precision: 0.0002, Recall: 0.0003\n",
            "F1: 0.0002, NDCG: 0.0361, MAP: 0.0001\n",
            "@k=20: Precision: 0.0003, Recall: 0.0010\n",
            "F1: 0.0005, NDCG: 0.0560, MAP: 0.0001\n",
            "Epoch 4/5: Train Loss: 0.3048, Val Loss: 0.3157\n",
            "Sample: Precision@10: 0.0002, NDCG@10: 0.0361\n",
            "\n",
            "Evaluating metrics at epoch 5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [17:25<00:00, 209.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification metrics:\n",
            "Accuracy:  0.9758\n",
            "Precision: 0.0000\n",
            "Recall:    0.0000\n",
            "F1-score:  0.0000\n",
            "AUC-ROC:   0.5024\n",
            "\n",
            "Ranking metrics:\n",
            "@k=5: Precision: 0.0016, Recall: 0.0013\n",
            "F1: 0.0014, NDCG: 0.0263, MAP: 0.0007\n",
            "@k=10: Precision: 0.0010, Recall: 0.0017\n",
            "F1: 0.0013, NDCG: 0.0368, MAP: 0.0006\n",
            "@k=20: Precision: 0.0013, Recall: 0.0050\n",
            "F1: 0.0021, NDCG: 0.0570, MAP: 0.0008\n",
            "Epoch 5/5: Train Loss: 0.3048, Val Loss: 0.3146\n",
            "Sample: Precision@10: 0.0010, NDCG@10: 0.0368\n",
            "\n",
            "Final evaluation on TEST set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final test metrics (subset):\n",
            "  Precision@10: 0.0014\n",
            "  Recall@10:    0.0024\n",
            "  NDCG@10:      0.0362\n",
            "  AUC-ROC:      0.5003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = (pd.Series(final_metrics)\n",
        "        .rename_axis('metric_at_k')\n",
        "        .reset_index(name='value'))\n",
        "df.sort_values('metric_at_k', ascending=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "TImI55WfMhkg",
        "outputId": "ba4c492b-37fa-4464-932c-e55f14e2ec16"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     metric_at_k     value\n",
              "1       recall@5  0.001603\n",
              "11     recall@20  0.003292\n",
              "6      recall@10  0.002405\n",
              "17        recall  0.000000\n",
              "0    precision@5  0.002041\n",
              "10  precision@20  0.001020\n",
              "5   precision@10  0.001429\n",
              "16     precision  0.000000\n",
              "3         ndcg@5  0.025598\n",
              "13       ndcg@20  0.056112\n",
              "8        ndcg@10  0.036213\n",
              "4          map@5  0.000878\n",
              "14        map@20  0.000811\n",
              "9         map@10  0.000752\n",
              "2           f1@5  0.001762\n",
              "12         f1@20  0.001539\n",
              "7          f1@10  0.001757\n",
              "18            f1  0.000000\n",
              "19       auc_roc  0.500288\n",
              "15      accuracy  0.976754"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2ba74fa-f3c5-4d33-ba17-dcc6d5d32717\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>metric_at_k</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>recall@5</td>\n",
              "      <td>0.001603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>recall@20</td>\n",
              "      <td>0.003292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>recall@10</td>\n",
              "      <td>0.002405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>recall</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>precision@5</td>\n",
              "      <td>0.002041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>precision@20</td>\n",
              "      <td>0.001020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>precision@10</td>\n",
              "      <td>0.001429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>precision</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ndcg@5</td>\n",
              "      <td>0.025598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ndcg@20</td>\n",
              "      <td>0.056112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ndcg@10</td>\n",
              "      <td>0.036213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>map@5</td>\n",
              "      <td>0.000878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>map@20</td>\n",
              "      <td>0.000811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>map@10</td>\n",
              "      <td>0.000752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>f1@5</td>\n",
              "      <td>0.001762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>f1@20</td>\n",
              "      <td>0.001539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>f1@10</td>\n",
              "      <td>0.001757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>f1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>auc_roc</td>\n",
              "      <td>0.500288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>accuracy</td>\n",
              "      <td>0.976754</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2ba74fa-f3c5-4d33-ba17-dcc6d5d32717')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c2ba74fa-f3c5-4d33-ba17-dcc6d5d32717 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c2ba74fa-f3c5-4d33-ba17-dcc6d5d32717');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-62ef6145-2b75-4ea6-962d-a3d3712496e9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-62ef6145-2b75-4ea6-962d-a3d3712496e9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-62ef6145-2b75-4ea6-962d-a3d3712496e9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"metric_at_k\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"recall@5\",\n          \"f1\",\n          \"f1@20\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2383273438201861,\n        \"min\": 0.0,\n        \"max\": 0.9767543903387617,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          0.0016034985422740521,\n          0.003292031098153547,\n          0.0561123050664768\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}